{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49eaea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a38490",
   "metadata": {},
   "source": [
    "## Extraction of hate/non-hate text spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dbba7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "num_repetitions = 1  # number of times to call GPT-4o. The final result averages the results\n",
    "max_tries = 3\n",
    "API_KEY = \"\"      # enter your API key\n",
    "client = OpenAI(\n",
    "   api_key=API_KEY\n",
    ")\n",
    "\n",
    "system_instruction = \"\"\"You are an expert human rater trained to label hateful messages after analysing the entire \n",
    "message. Also, you are expert for the task of identifying and extracting terms in the messages relevant for hate or love\"\"\"\n",
    "\n",
    "def label_span(message, target_identity, model='gpt-3.5-turbo'):\n",
    "  attempt = 0\n",
    "  user_instruction = f\"\"\"Identify bare minimum length text span (a phrase that uses minimum number of words) in \n",
    "  the message that expresses hate or love towards the {target_identity}. Remove insignificant words from the text \n",
    "  span other than that essentially required to express hate or love.\n",
    "  Only if the text span does not reflect the sentiment of entire message, modify the text span accordingly by adding \n",
    "  negate words like \"not\" or \"no\" to that minimum text span so that it reflects the sentiment of entire message and \n",
    "  then return that modified text span without mentioning {target_identity}. Answer 'None' if you can't find any. \n",
    "  For example, Message: \"No immigrants should be denied access to healthcare\". Here, the sentiment is love \n",
    "  towards immigrants. Therefore, the original text span \"should be denied access‚Äù should be negated \n",
    "  so that it reflects the message sentiment. i.e., the modified text span has to be \"should not be denied access\". \n",
    "  If the original text span already aligns with the sentiment expressed in the entire message, no modification is necessary.  Now try to find the text span for me  that reflects the message. Just return the \n",
    "  final answer.\n",
    "\\n\\n Message: `{message}`.\"\"\"\n",
    "\n",
    "  while attempt < max_tries:\n",
    "    try:\n",
    "      completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "          temperature = 0.2,\n",
    "        messages=[\n",
    "          {\"role\": \"system\", \"content\": system_instruction},\n",
    "          {\"role\": \"user\", \"content\": user_instruction}\n",
    "        ]\n",
    "      )\n",
    "      response = completion.choices[0].message.content\n",
    "      return response\n",
    "    except openai.RateLimitError as e:\n",
    "      print(f\"OpenAI API request exceeded rate limit: {e}\")\n",
    "      time.sleep(20)\n",
    "      pass\n",
    "    except openai.APIConnectionError as e:\n",
    "      print(f\"Failed to connect to OpenAI API: {e}\")\n",
    "      time.sleep(20)\n",
    "      pass\n",
    "    except openai.APIError as e:\n",
    "      print(f\"OpenAI API returned an API Error: {e}\")\n",
    "      time.sleep(20)\n",
    "      pass\n",
    "    except ValueError as e:\n",
    "      print(f\"[WARNING] ValueError: {e}.\")\n",
    "      pass\n",
    "    attempt += 1\n",
    "  return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439c9ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = \"No black person should experience racism.\" \n",
    "target_identity = \"black people\"\n",
    "label_span(message, target_identity, model='gpt-4o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78551335",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = Path(\"/Users/hatecheck/gpt4_aspects/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4719d4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_functionalities = [\"F11: Non-hateful use of profanity\",\n",
    "                          \"F22: Abuse targeted at objects\",\n",
    "                          \"F23: Abuse targeted at individuals (not as member of a prot. group)\",\n",
    "                          \"F24: Abuse targeted at nonprotected groups (e.g. professions)\",\n",
    "                          \"F25-29: Spelling variation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665ee72e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for p in dataset_path.glob('*.csv'):\n",
    "    target_identity =  re.search('dataset_(.+?).csv', p.name).group(1)\n",
    "    df = pd.read_csv(p)\n",
    "    df = df[~df['functionality'].isin(ignore_functionalities)]\n",
    "    messages = df['message'].tolist()\n",
    "    functionalities = df['functionality'].tolist()\n",
    "    hate_labels = df['hate_label'].tolist()\n",
    "    text_spans = []\n",
    "    print(f\"Text span for {target_identity} ....\")\n",
    "    for i in trange(len(messages)):\n",
    "        text_span = label_span(messages[i], target_identity, model='gpt-4o')\n",
    "        text_spans.append(text_span)\n",
    "    j+=1\n",
    "    df['text_spans'] = text_spans\n",
    "    df.to_csv(dataset_path/f\"text_spans/text_span_{target_identity}.csv\", index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52580f9",
   "metadata": {},
   "source": [
    "## Embedding of spans using openAI embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4bfbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ai_embedding(text, model=\"text-embedding-3-large\"): \n",
    "    return client.embeddings.create(input = [text], model=model).data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0049682d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_enclosing_quotes(s):\n",
    "    # Check for and remove enclosing double quotes\n",
    "    if s.startswith('\"') and s.endswith('\"'):\n",
    "        s = s[1:-1]\n",
    "    # Check for and remove enclosing backticks\n",
    "    elif s.startswith('`') and s.endswith('`'):\n",
    "        s = s[1:-1]\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ea9574",
   "metadata": {},
   "outputs": [],
   "source": [
    "span_path = Path(\"/Users/hatecheck/gpt4_aspects/text_spans\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f42bb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in span_path.glob('*.csv'):\n",
    "    target_identity =  re.search('text_span_(.+?).csv', p.name).group(1)\n",
    "    df = pd.read_csv(p)\n",
    "    df['text_spans'] = df['text_spans'].apply(lambda span: remove_enclosing_quotes(span))\n",
    "    df_cleaned = df[df['text_spans'] != 'None']\n",
    "    df_cleaned.to_csv(span_path/f\"text_span_cleaned_{target_identity}.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b98ac5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "entire_embed = []\n",
    "for p in span_path.glob('*.csv'):\n",
    "    target_identity =  re.search('text_span_(.+?).csv', p.name).group(1)\n",
    "    df = pd.read_csv(p)\n",
    "    text_spans = df['text_spans'].tolist()\n",
    "    hate_labels = df['hate_label'].tolist()\n",
    "    print(f\"Text span for {target_identity} ....\")\n",
    "    embeddings = []\n",
    "    for i in trange(len(text_spans)):\n",
    "        embeddings.append(get_ai_embedding(text_spans[i]))\n",
    "    final_embed = np.array(embeddings)\n",
    "    final_embed_reshaped = final_embed.reshape(final_embed.shape[0], -1)\n",
    "    df_embed = pd.DataFrame(np.array(final_embed_reshaped))\n",
    "    df_embed[\"text_span\"] = text_spans\n",
    "    df_embed[\"target_identity\"] = target_identity\n",
    "    df_embed[\"hate_labels\"] = hate_labels\n",
    "    entire_embed.append(df_embed)\n",
    "df_entire_embed = pd.concat(entire_embed, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734e74db",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e82bb7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Using t-SNE to reduce to 2 dimensions for visualization\\\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "df = df_entire_embed[df_entire_embed[\"hate_labels\"]==0]\n",
    "embeddings = np.array(df.drop(['target_identity', 'text_span', 'hate_labels'], axis=1))\n",
    "labels = df['target_identity'].values\n",
    "\n",
    "#pca = PCA(n_components=200)\n",
    "#embeddings_pca = pca.fit_transform(embeddings)\n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=30, n_iter=300, random_state=0)\n",
    "embeddings_tsne = tsne.fit_transform(embeddings)\n",
    "\n",
    "kmeans = KMeans(n_clusters=50, random_state=0)\n",
    "clusters = kmeans.fit_predict(embeddings_tsne)\n",
    "\n",
    "df['Component 1'] = embeddings_tsne[:, 0]\n",
    "df['Component 2'] = embeddings_tsne[:, 1]\n",
    "df['Cluster'] = clusters\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(\n",
    "    x='Component 1', y='Component 2',\n",
    "    hue='target_identity', style='Cluster',\n",
    "    palette='bright', data=df,\n",
    "    s=100, alpha=1, edgecolor='w'\n",
    ")\n",
    "plt.title('t-SNE visualization of embeddings with category labels and K-Means clusters')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8a5eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df[[\"text_span\", \"target_identity\", \"Cluster\"]]\n",
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bd1e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_distribution = df_new.groupby(['Cluster', 'target_identity']).size().unstack().fillna(0)\n",
    "label_distribution.plot(kind='bar', stacked=True, figsize=(10, 6))\n",
    "\n",
    "plt.title('Target_identity Distribution in Each Cluster')\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(title='target_identity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb14fb5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate the distribution of labels within each cluster\n",
    "\n",
    "label_distribution = df_new.groupby(['Cluster', 'target_identity']).size().unstack().fillna(0)\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(15,8), dpi=400)\n",
    "sns.heatmap(label_distribution, annot=True, fmt='.0f', annot_kws={\"size\": 8},  cmap='viridis')\n",
    "\n",
    "plt.xticks(fontsize=6)\n",
    "plt.yticks(fontsize=6)\n",
    "plt.title('Heatmap of Label Distribution in Each Cluster: OpenAI')\n",
    "plt.xlabel('Target_identity')\n",
    "plt.ylabel('Cluster')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551fc341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NLTK data (if not already downloaded)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8047ad5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding the frequency of text spans for each category\n",
    "\n",
    "sorted_counts_A = {}\n",
    "for each in df_new[\"target_identity\"].unique():\n",
    "    df = df_new[df_new[\"target_identity\"]==each]\n",
    "    counter_A = Counter(df['text_span'])\n",
    "    sorted_counts_A[each] = sorted(counter_A.items(), key=lambda x: x[1], reverse=True)\n",
    "    print(f\"\\nSorted counts of unique values for {each}\")\n",
    "    print(sorted_counts_A[each])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
